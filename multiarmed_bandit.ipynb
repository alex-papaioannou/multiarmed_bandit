# Loading the data

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

import scipy.stats as spst
from scipy.optimize import minimize
import scipy.special as spsp
from statsmodels.formula.api import ols
import warnings
warnings.simplefilter("ignore")

# csv file stored in Google Drive

link="https://drive.google.com/open?id=1Wk0OcUBAXZw5-0ugL7y6Rioj8PMp8n0m"

_,id=link.split("=")
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('Ccat.csv')  

Ccat = pd.read_csv('Ccat.csv')
Ccat.head(10)

# Regret means the difference in the benefit between selecting the best policy we used when doing the experiment.

# For example, in this example, we can compute the number of customers we retained after 7 days using the experiment. We can also compute the number of customers we could have retained after 7 days using the best policy. The difference in the numbers is called regret. When different experiment could tell us the consistent result, a good experiment will be one with a lower regret.

#total gamerounds from A/B testing

np.sum(Ccat['sum_gamerounds'].values)

#total game rounds if the best policy was carried out

52.456264*Ccat.shape[0]

#regret

regret=52.456264*Ccat.shape[0]-np.sum(Ccat['sum_gamerounds'].values)

## Multi-armed Bandit

# In the previous example, the A/B testing is carried out in one-wave. We randomly assigned around 50% of the customers to group 1, and 50% of the customers to group 2. The result, unfortunately suggests that moving to the gate to Level 40 does not sound like a good idea. Now the question is, if a new policy is apparently different, do we still have to assign 50% customers to each group. Is there a way for us to assign more customers to the better group so we can benefit early from the experiment?

# Multi-arm Bandit is a simple reinforcement learning algorithm that can help us achieve this goal. It carries exploration and exploitation at the same time. The experiment is carried out in waves. From each wave, we will keep on updating our brief of which policy is better. We will simply assign the better group with more customers.
# The detailed steps for our experiment is as follows:

# Step 1: We will initialize our algorithm on a small group of customers
# Step 2: After 7 days, we compute the retention rate in both groups.
# Step 3: At this moment, we have some preliminary knowledge of which group is better. In the next wave of experiment, we should assign more customers in the "better" group (exploitation). At the same time, we still need to assign a certain amount of customer into the other group so that we have a better estimation of the retention rate (exploration).
# We can set an exploration parameter  𝜖  (for example 0.2). We will then assign each customer to the "better" group with  𝑝=1−𝜖 .
# Step 4: After a new wave of experiment, we update the retention rate for each group. Because we have more samples, the "better" group might stay the same or might switch. In the next wave, we will, again send each customer to the "better" group with  𝑝=1−𝜖 .
# Step 5: We will do this many times until we think that the result is conclusive.
# In this exercise, we assume that the samples we have follows the market distribution.
# If we assign 50 users to receive the treatment, we will simply draw 50 users from the users with version="Gate40" and use their outcome.
# We assume that in the first wave, we will assign 6189 people.
# In the follow up wave, will assign 4000 people in each wave with epsilon=0.1 and do additional 21 waves.

Gate30segment=Ccat[Ccat["version"]=="gate_30"]
Gate40segment=Ccat[Ccat["version"]=="gate_40"]

# Now, let's look at the result of the experiment and the regret.

def multiarm():
  N=6189
  N1=np.random.binomial(N,0.5)
  N2=N-N1
  samples1=Gate30segment.sample(N1)
  samples2=Gate40segment.sample(N2)
  gamerounds30=samples1["sum_gamerounds"].values
  gamerounds40=samples2["sum_gamerounds"].values
  p_explore=0.1
  for i in range(21):
    N=4000
    if np.mean(gamerounds30)>np.mean(gamerounds40):
      N1=np.random.binomial(N,1-p_explore)
    else:
      N1=np.random.binomial(N,p_explore)
    N2=N-N1
    samples1=Gate30segment.sample(N1)
    samples2=Gate40segment.sample(N2)
    gamerounds30=np.append(gamerounds30,\
                            samples1["sum_gamerounds"].values)
    gamerounds40=np.append(gamerounds40,\
                            samples2["sum_gamerounds"].values)
  if np.mean(gamerounds30)>np.mean(gamerounds40):
    optimal=np.mean(gamerounds30)*Ccat.shape[0]
  else:
    optimal=np.mean(gamerounds40)*Ccat.shape[0] 
  regret=optimal-(np.sum(gamerounds30)+np.sum(gamerounds40))
  return regret
regret_array=[multiarm() for i in range(100)]

plt.hist(np.array(regret_array))

np.mean(regret_array)
